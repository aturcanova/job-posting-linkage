{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean JobPostings Data\n",
    "\n",
    "In this notebook, we clean the JobPostings dataset.\n",
    "\n",
    "The notebook is organized in the following fashion:\n",
    "\n",
    "0. Import libraries and define constants\n",
    "1. Load Job Posting dataset\n",
    "2. Check the data\n",
    "3. Create a new dataframe with significant columns\n",
    "4. Clean company names\n",
    "5. Addresses\n",
    "6. Clean addresses\n",
    "7. Translate English names in addresses\n",
    "8. German ZIP codes\n",
    "9. Fill missing company addresses\n",
    "10. Fill missing job addresses\n",
    "11. Join dataframes\n",
    "12. Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "import linkage.model.fill_addresses as fa\n",
    "import linkage.model.german_zip_codes as gzc\n",
    "\n",
    "from linkage.model.utils import read_dataframe, save_dataframe\n",
    "from linkage.model.change_dataframe import replace_german_characters, repair_broken_unicode\n",
    "from linkage.model.change_dataframe import replace_other_latin_characters\n",
    "from linkage.model.clean_names import clean_names, clean_names_with_dictionary\n",
    "from linkage.model.clean_addresses import clean_addresses, replace_english_names\n",
    "from linkage.model.examine_dataframe import contains_all_nan, contains_any_nan, drop_all_nan, count_redundant_spaces\n",
    "from linkage.model.examine_dataframe import column_contains_nan, drop_subset_nan, print_dataframe_length\n",
    "from linkage.visualize.plot import plot_histogram\n",
    "from linkage.visualize.visualize_dataframe import show_nan_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'std' for standardized, 'std_dict_40k' for dictionary cleaning with the 40k most common words\n",
    "NOTE = 'std'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify paths to data directories\n",
    "RAW_DATA_DIR = '../data/raw/jobpostings'\n",
    "INTERMEDIATE_DATA_DIR = \"../data/intermediate/jobpostings\"\n",
    "PROCESSED_DATA_DIR = \"../data/processed/jobpostings\"\n",
    "\n",
    "# Specifie file names\n",
    "JP_PROCESSED_FILE = f\"jobpostings_test_sample_{NOTE}.csv\"\n",
    "\n",
    "# List of files containing job postings\n",
    "JP_FILES_LIST = ['jobpostings_test_sample.txt', \n",
    "                 'jobpostings_test_sample2.txt']\n",
    "\n",
    "# Dataframe's index\n",
    "JP_INDEX = 'jobposting_id'\n",
    "\n",
    "# Column names\n",
    "# Good to specify if the column names would change\n",
    "COMPANY_NAME = 'company'\n",
    "COMPANY_CITY, COMPANY_ZIP, COMPANY_STATE = 'company_city', 'company_zipcode', 'company_state'\n",
    "JOB_CITY, JOB_ZIP, JOB_STATE = 'job_city', 'job_zipcode', 'job_state'\n",
    "\n",
    "# Columns to take when reading the dataframe from a file\n",
    "USEFUL_COLS = [JP_INDEX, COMPANY_NAME, \n",
    "               COMPANY_CITY, COMPANY_ZIP, COMPANY_STATE, \n",
    "               JOB_CITY, JOB_ZIP, JOB_STATE]\n",
    "\n",
    "# Address columns\n",
    "COMPANY_ADDR_COLS = [COMPANY_CITY, COMPANY_ZIP, COMPANY_STATE]\n",
    "JOB_ADDR_COLS = [JOB_CITY, JOB_ZIP, JOB_STATE]\n",
    "\n",
    "# Split columns to lists if numerical or alpha-numerical\n",
    "COMPANY_ADDR_COLS_NAMES = [COMPANY_CITY, COMPANY_STATE]\n",
    "COMPANY_ADDR_COLS_ZIPCODES = [COMPANY_ZIP]\n",
    "\n",
    "JOB_ADDR_COLS_NAMES = [JOB_CITY, JOB_STATE]\n",
    "JOB_ADDR_COLS_ZIPCODES = [JOB_ZIP]\n",
    "\n",
    "# Additional columns\n",
    "COMPANY_NAME_STANDARDIZED = 'company_standard'\n",
    "COMPANY_NAME_DICT_CLEANED = 'company_dict_clean'\n",
    "\n",
    "# Labels for plots\n",
    "PLOT_LABELS = ['Comp. name', 'Comp. city', 'Comp. ZIP code', 'Comp. state', 'Job. city', 'Job. ZIP code', 'Job. state']\n",
    "PLOT_LABELS_WITH_DICT_CLEAN = ['Comp. name', 'Comp. city', 'Comp. ZIP code', 'Comp. state', \n",
    "                               'Job. city', 'Job. ZIP code', 'Job. state', 'Comp. name stand.', 'Comp. name dict. clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load JobPostings dataset\n",
    "\n",
    "The Job Posting dataset is stored on path:\n",
    "```python\n",
    "../data/raw/jobpostings/\n",
    "```\n",
    "The file containing dataset is named _jobpostings_test_sample.txt_.\n",
    "\n",
    "The data are read into Pandas **DataFrame**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Here we save the read files for easy concatenation into a single dataframe\n",
    "df_list = []\n",
    "\n",
    "# Iterate over job postings files and read them to dataframes\n",
    "for jobpostings_file in JP_FILES_LIST:\n",
    "    df_part = read_dataframe(RAW_DATA_DIR, jobpostings_file, JP_INDEX, USEFUL_COLS, dtype=str)\n",
    "    df_list.append(df_part)\n",
    "\n",
    "# Concatenate dataframes to one main dataframe\n",
    "df = pd.concat(df_list)\n",
    "\n",
    "print(f\"Num. of records: {len(df)}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check the data\n",
    "\n",
    "What should be checked:\n",
    "- Columns' type\n",
    "- Number of unique rows\n",
    "- Index\n",
    "- NaN values\n",
    "- Broken Unicode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the dataframe info\n",
    "\n",
    "First, we check the number of columns and rows.\n",
    "\n",
    "We print the column names with their data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names, the number of the columns, the number of rows\n",
    "df.info(verbose=True , show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for uniqueness and index\n",
    "\n",
    "Then, we look at the uniqueness of values in the individual columns.\n",
    "\n",
    "Next, we check if the data frame has an index. If there is no index, the execution ends with an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the column is unique\n",
    "for i in df.columns:\n",
    "    print(f'{i} is unique: {df[i].is_unique}') # TODO: only print unique\n",
    "\n",
    "# Check the index values\n",
    "# Results in error if there is no index\n",
    "df.index.values  # Remember, we set the index at the beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check NaN values\n",
    "\n",
    "Here, we check the missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_nan_counts(df, PLOT_LABELS, ymin=0, ymax=len(df)+500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All values are NaN\n",
    "\n",
    "Let's check if some rows are NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_all_nan(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deal with all NaN rows\n",
    "\n",
    "For now, we will drop the rows with only NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_all_nan(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Company name is NaN\n",
    "Let's check if some of the company names are NaN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_contains_nan(df, COMPANY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deal with NaN company name values\n",
    "\n",
    "For now, we will drop the rows with NaN company names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_subset_nan(df, COMPANY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check broken Unicode\n",
    "\n",
    "It can happen that someone has encoded Unicode with one standard and decoded it with a different one.\n",
    "\n",
    "As a result, some of the characters may be \"broken\".\n",
    "\n",
    "A nice example is ampersand (&) which will decode as &amp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for the broken ampersand\n",
    "df[df[COMPANY_NAME].str.contains('&amp;', regex=True, case=False) == True].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Repair broken Unicode\n",
    "\n",
    "The library ftfy (fixes text for you) will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Repair broken unicode\n",
    "repair_broken_unicode(df, df.columns)\n",
    "\n",
    "# Look again for the broken ampersand\n",
    "df[df[COMPANY_NAME].str.contains('&amp;', regex=True, case=False) == True].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace with basic Latin characters\n",
    "\n",
    "Let's check if the dataframe contains any characters other than basic Latin ones and replace them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all rows with other than German alphanumerical characters\n",
    "df[df[COMPANY_NAME].str.contains('[ÄÖÜßÁÉÓÚ]', regex=True) == True].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace German characters\n",
    "\n",
    "Replace German characters with umlaut and ß with their basic Latin equivalents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace characters with umlaut\n",
    "replace_german_characters(df, df.columns)\n",
    "\n",
    "# Check all rows with other than German alphanumerical characters\n",
    "df[df[COMPANY_NAME].str.contains('[ÁÉÓÚ]', regex=True) == True].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Columns contain only German characters\n",
    "\n",
    "Let's check if the company names contain different than German characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace á to a etc.\n",
    "replace_other_latin_characters(df, df.columns)\n",
    "\n",
    "# Check all rows with other than latin alphanumerical characters\n",
    "df[df[COMPANY_NAME].str.contains('[ÁÉÓÚ]', regex=True) == True].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a new dataframe with significant columns\n",
    "\n",
    "Let's create a new dataframe to simplify standardization and cleaning of company names and addresses.\n",
    "\n",
    "Taken columns for __company name__ standardization:\n",
    "- company_id\n",
    "- company\n",
    "\n",
    "Taken columns for __company addresses__ standardization:\n",
    "- company_zipcode\n",
    "- company_city\n",
    "- company_state\n",
    "\n",
    "Taken columns for __job addresses__ standardization:\n",
    "- job_zipcode\n",
    "- job_city\n",
    "- job_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe for name cleaning\n",
    "\n",
    "Create a new dataframe with significant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe for name cleaning\n",
    "# Take jobposting_id and company name\n",
    "name_df = df[[COMPANY_NAME]].copy()\n",
    "name_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe for addresses\n",
    "\n",
    "Create new dataframes for company addresses and job addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save address part of dataframe for later\n",
    "company_addr_df = df[COMPANY_ADDR_COLS].copy()\n",
    "job_addr_df = df[JOB_ADDR_COLS].copy()\n",
    "\n",
    "print(f'Company address dataframe:\\n{company_addr_df.head()}\\n\\n')\n",
    "\n",
    "print(f'Job address dataframe:\\n{job_addr_df.head()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the range of the values\n",
    "\n",
    "We look at the values of company names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(name_df, column_name=COMPANY_NAME, title='Count by company name before standardization.', \n",
    "               ylabel='Num. of appearances per company name', xlabel='Company name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Clean company names\n",
    "\n",
    "Here, we will clean the company names.\n",
    "\n",
    "In general, we will remove:\n",
    "- Redundant characters\n",
    "- Redundant individually standing numbers\n",
    "- Redundant white characters\n",
    "\n",
    "We will apply:\n",
    "- PDP standardization routines\n",
    "- Dictionary cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for corporation names, e.g. GmBH\n",
    "name_df[name_df[COMPANY_NAME].str.contains(' GMBH ', regex=True, case=False)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check for some special character in COMPANY_NAME column\n",
    "name_df[name_df[COMPANY_NAME].str.contains('[&]', regex=True)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Remove redundant words\n",
    "\n",
    "Here, we standardize company names and apply PDP: \n",
    "\n",
    "PDP standardization routines:\n",
    "\n",
    "0. Remove non-alphanumerical characters.\n",
    "\n",
    "1. Change things to shortcuts\n",
    "\n",
    "2. Remove the shortcuts\n",
    "\n",
    "3. Remove corporate names and non-corporate\n",
    "\n",
    "4. Combine abbreviations and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "name_df[COMPANY_NAME_STANDARDIZED] = name_df[COMPANY_NAME]\n",
    "name_df.head()\n",
    "\n",
    "# Clean company names from redundant words\n",
    "clean_names(name_df, column_name=COMPANY_NAME_STANDARDIZED)\n",
    "\n",
    "name_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check empty names after cleaning\n",
    "\n",
    "Let's check if some of the values resulted in empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which company names resulted into empty string after cleaning\n",
    "empty_name_filter = name_df[name_df[COMPANY_NAME_STANDARDIZED] == '']\n",
    "\n",
    "empty_name_df = name_df[name_df.index.isin(empty_name_filter.index)]\n",
    "\n",
    "empty_name_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill empty \n",
    "\n",
    "Fill the empty company name values with their original version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the values where the empty company name is\n",
    "empty_name_df[COMPANY_NAME_STANDARDIZED] = empty_name_df[COMPANY_NAME]\n",
    "\n",
    "# Clean company names without removing redundant words\n",
    "clean_names(empty_name_df, column_name=COMPANY_NAME_STANDARDIZED, remove_redundant=False)\n",
    "\n",
    "# Update the values where the empty company name is\n",
    "name_df.update(empty_name_df)\n",
    "\n",
    "name_df[name_df.index.isin(empty_name_filter.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Company names do not contains space on the beginning and the end, or double spaces\n",
    "\n",
    "Cleaning of the names should have removed all the redundant spaces created during cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_redundant_spaces(name_df, COMPANY_NAME_STANDARDIZED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check appearence of different types of companies\n",
    "name_df[name_df[COMPANY_NAME_STANDARDIZED].str.contains('consult', regex=True, case=False) == True].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove redundant words using dictionary\n",
    "\n",
    "We use a dictionary containing the 40k most common German words.\n",
    "\n",
    "We try to remove words which do not belong to the company names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_df[COMPANY_NAME_DICT_CLEANED] = name_df[COMPANY_NAME_STANDARDIZED]\n",
    "name_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Clean names with a dictionary\n",
    "# clean_names_with_dictionary(name_df, column_name=COMPANY_NAME_DICT_CLEANED) # TODO: uncomment\n",
    "name_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_df[name_df['company_dict_clean'] == 'DB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Check the result\n",
    "\n",
    "Let's plot value counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Counts after standardization\n",
    "company_counts_df = name_df[COMPANY_NAME_STANDARDIZED].value_counts().copy()\n",
    "plot_histogram(company_counts_df, title='Count by company name after standardization.', \n",
    "               ylabel='Num. of appearances per company name', xlabel='Company name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Counts after standardization and dictionary cleaning\n",
    "company_dict_clean_counts_df = name_df[COMPANY_NAME_DICT_CLEANED].value_counts().copy()\n",
    "plot_histogram(company_dict_clean_counts_df, title='Count by company name after standardization and dictionary cleaning.', \n",
    "               ylabel='Num. of appearances per company name', xlabel='Company name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Show unique values of company names after dictionary cleaning\n",
    "name_df[name_df[COMPANY_NAME_DICT_CLEANED] == ''][COMPANY_NAME].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Save company appearences to CSV\n",
    "\n",
    "To check possible missed company types, e.g. GGMBH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "company_standard_counts_df_file = \"jobpostings_company_standard_value_counts.csv\"\n",
    "company_dict_clean_counts_df_file = \"jobpostings_company_copy_value_counts.csv\"\n",
    "\n",
    "# Save dataframe to a csv file\n",
    "save_dataframe(company_counts_df, INTERMEDIATE_DATA_DIR, company_standard_counts_df_file)\n",
    "save_dataframe(company_dict_clean_counts_df, INTERMEDIATE_DATA_DIR, company_dict_clean_counts_df_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Update the main dataframe\n",
    "\n",
    "Update the original dataframe by adding standardized and dict. cleaned company names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df[COMPANY_NAME_STANDARDIZED] = name_df[COMPANY_NAME_STANDARDIZED]\n",
    "df[COMPANY_NAME_DICT_CLEANED] = name_df[COMPANY_NAME_DICT_CLEANED]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Addresses\n",
    "\n",
    "Process the addresses for Record Linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check NaN values\n",
    "\n",
    "Some addresses my contain NaN values. We will try to fill them when possible. \n",
    "\n",
    "We will drop Addresses which contain only NaN values. These records have non-NaN company name, otherwise they would be dropped at the beginning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All values are NaN\n",
    "\n",
    "Let's check if some rows are NaN. First, check company addresses, then job addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Company address contains only NaN\n",
    "contains_all_nan(company_addr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Job address contains only NaN\n",
    "contains_all_nan(job_addr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deal with all NaN values\n",
    "\n",
    "Drop the rows, which are only NaN as we do not use the during the cleaning process. Removed rows are preserved in the main dataframe (as the respective company name is not NaN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "drop_all_nan(company_addr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "drop_all_nan(job_addr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some values are NaN\n",
    "\n",
    "Let's check rows with any NaN value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Company address contains any NaN\n",
    "contains_any_nan(company_addr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Job address contains any NaN\n",
    "contains_any_nan(job_addr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check State values\n",
    "\n",
    "Plot counts for each of the German states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot company states\n",
    "df[COMPANY_STATE].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot job states\n",
    "df[JOB_STATE].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clean addresses\n",
    "\n",
    "Clean non-numerical parts of addresses from non-alphabetical characters, group single consecutive letters, and turn names upper case.\n",
    "\n",
    "Standardization for addresses is similar to the one for company names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean company addresses\n",
    "\n",
    "Here, we standardize company addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Clean company addresses\n",
    "for column_name in COMPANY_ADDR_COLS_NAMES:\n",
    "\n",
    "    clean_addresses(company_addr_df, column_name)\n",
    "    \n",
    "company_addr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean job addresses\n",
    "\n",
    "Here, we standardize job addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Clean job addresses\n",
    "for column_name in JOB_ADDR_COLS_NAMES:\n",
    "\n",
    "    clean_addresses(job_addr_df, column_name)\n",
    "    \n",
    "job_addr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical \n",
    "\n",
    "After the standardization, state columns should have maximum of 17 different values (16 for Bundeslands and 1 for NaN values).\n",
    "Therefore, we can change the datatype for states to _categorical__ and spare some memory and computational time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Convert a column type to categorical to save memory\n",
    "company_addr_df[COMPANY_STATE] = company_addr_df[COMPANY_STATE].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Convert a column type to categorical to save memory\n",
    "job_addr_df[JOB_STATE] = job_addr_df[JOB_STATE].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Translate English names in addresses\n",
    "\n",
    "Some of the cities may be named in English. \n",
    "Translate the English names to their German equivalents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Check for German city name\n",
    "company_addr_df[company_addr_df[COMPANY_CITY].str.contains('MUNICH') == True].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Translate company city\n",
    "replace_english_names(company_addr_df, COMPANY_CITY)\n",
    "\n",
    "# Translate job city\n",
    "replace_english_names(job_addr_df, JOB_CITY)\n",
    "\n",
    "# Check for German city name\n",
    "company_addr_df[company_addr_df[COMPANY_CITY].str.contains('MUNICH') == True].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_addr_df[company_addr_df[COMPANY_CITY].str.contains('[^a-zA-Z0-9ÜÄÖß ]', regex=True) == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. German ZIP codes\n",
    "\n",
    "The _German-Zip-Codes.csv_ of German ZIP codes is saved on path\n",
    "```python\n",
    "../data/external/german-zip-codes\n",
    "```\n",
    "\n",
    "We use German-Zip-Codes to fill the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Initialize class for German-Zip-Codes\n",
    "german_zipcodes = gzc.GermanZipCodes()\n",
    "\n",
    "# Read the standardized dataframe of german zip codes\n",
    "zip_df = german_zipcodes.zip_df\n",
    "zip_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Check the german-zip-codes dataframe info\n",
    "zip_df.info(verbose=True , show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace the mean of ZIP codes\n",
    "\n",
    "Because we used the mean of the ZIP codes in the previous step, we do not want to use the full ZIP to avoid confusion.\n",
    "\n",
    "We replace the last 3 numbers of ZIP codes with 'xxx'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "zip_mean_df = german_zipcodes.zip_mean_df\n",
    "zip_mean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Fill missing company addresses\n",
    "\n",
    "Here, we try to fill missing parts of company addresses using other non-missing values of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize class for cleaning data\n",
    "fill_address = fa.FillAddress(company_addr_df, zip_df, zip_mean_df, COMPANY_ZIP, COMPANY_CITY, COMPANY_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZIP codes\n",
    "\n",
    "Fill missing company ZIP codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Filter missing or invalid zipcodes and create a new dataframe\n",
    "missing_zip_mask = ((company_addr_df[COMPANY_ZIP].isna() | company_addr_df[COMPANY_ZIP].str.contains('[a-zA-Z]', regex=True)) \\\n",
    "                   & company_addr_df[COMPANY_CITY].notna())\n",
    "\n",
    "missing_zip_df = company_addr_df[missing_zip_mask].copy()\n",
    "\n",
    "print_dataframe_length(missing_zip_df)\n",
    "missing_zip_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Fill missing zipcode\n",
    "missing_zip_df = fill_address.fill_missing_zipcode(missing_zip_df)\n",
    "\n",
    "column_contains_nan(missing_zip_df, COMPANY_ZIP, print_df=False)\n",
    "missing_zip_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Update company addresses\n",
    "company_addr_df.update(missing_zip_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "company_addr_df[company_addr_df[COMPANY_ZIP].isna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Company city\n",
    "\n",
    "Fill missing company cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Filter missing or invalid cities and create a new dataframe\n",
    "missing_city_mask = (company_addr_df[COMPANY_CITY].isna() \\\n",
    "                     | (company_addr_df[COMPANY_CITY].str.contains('[^A-Z ]', regex=True) == True)) \\\n",
    "                     & company_addr_df[COMPANY_ZIP].notna()\n",
    "\n",
    "missing_city_df = company_addr_df[missing_city_mask].copy()\n",
    "\n",
    "print_dataframe_length(missing_city_df)\n",
    "missing_city_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Fill missing city\n",
    "missing_city_df = fill_address.fill_missing_city(missing_city_df)\n",
    "\n",
    "column_contains_nan(missing_city_df, COMPANY_CITY, print_df=False)\n",
    "missing_city_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Update company addresses\n",
    "company_addr_df.update(missing_city_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "company_addr_df[company_addr_df[COMPANY_CITY].isna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German state\n",
    "\n",
    "Fill missing company states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Filter missing or invalid states and create a new dataframe\n",
    "missing_state_mask = (company_addr_df[COMPANY_STATE].isna()\n",
    "                     | company_addr_df[COMPANY_STATE].str.contains('0-9', regex=True)\n",
    "                     | ~company_addr_df[COMPANY_STATE].isin(fill_address.bundesland_lst)) \\\n",
    "                     & (company_addr_df[COMPANY_ZIP].notna() \n",
    "                     | company_addr_df[COMPANY_CITY].notna())\n",
    "\n",
    "missing_state_df = company_addr_df[missing_state_mask].copy()\n",
    "\n",
    "print_dataframe_length(missing_state_df)\n",
    "missing_state_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Fill missing state\n",
    "missing_state_df = fill_address.fill_missing_state(missing_state_df)\n",
    "\n",
    "column_contains_nan(missing_state_df, COMPANY_STATE, print_df=False)\n",
    "missing_state_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Update company addresses\n",
    "company_addr_df.update(missing_state_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "company_addr_df[company_addr_df[COMPANY_STATE].isna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Schwadorf 2432 is situated near Vienna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Fill missing job addresses\n",
    "\n",
    "Here, we try to fill missing parts of job addresses using other non-missing values of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Initialize class for cleaning data\n",
    "fill_address = fa.FillAddress(job_addr_df, zip_df, zip_mean_df, JOB_ZIP, JOB_CITY, JOB_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zip codes\n",
    "\n",
    "Fill missing job ZIP codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Filter missing or invalid zipcodes and create a new dataframe\n",
    "missing_zip_mask = (job_addr_df[JOB_ZIP].isna() \\\n",
    "                   | job_addr_df[JOB_ZIP].str.contains('[a-zA-Z]', regex=True)) \\\n",
    "                   & job_addr_df[JOB_CITY].notna()\n",
    "\n",
    "missing_zip_df = job_addr_df[missing_zip_mask].copy()\n",
    "\n",
    "print_dataframe_length(missing_zip_df)\n",
    "missing_zip_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Fill missing zipcode\n",
    "missing_zip_df = fill_address.fill_missing_zipcode(missing_zip_df)\n",
    "\n",
    "column_contains_nan(missing_zip_df, JOB_ZIP, print_df=False)\n",
    "missing_zip_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Update job addresses\n",
    "job_addr_df.update(missing_zip_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "job_addr_df[job_addr_df[JOB_ZIP].isna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job city\n",
    "\n",
    "Fill missing job cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Filter missing or invalid cities and create a new dataframe\n",
    "missing_city_mask = (job_addr_df[JOB_CITY].isna() \\\n",
    "                     | (job_addr_df[JOB_CITY].str.contains('[0-9]', regex=True))) \\\n",
    "                     & (job_addr_df[JOB_ZIP].notna() \n",
    "                        | job_addr_df[JOB_STATE].notna())\n",
    "\n",
    "missing_city_df = job_addr_df[missing_city_mask].copy()\n",
    "\n",
    "print_dataframe_length(missing_city_df)\n",
    "missing_city_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Fill missing city\n",
    "missing_city_df = fill_address.fill_missing_city(missing_city_df)\n",
    "\n",
    "column_contains_nan(missing_city_df, JOB_CITY, print_df=False)\n",
    "missing_city_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Update job addresses\n",
    "job_addr_df.update(missing_city_df, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "job_addr_df[job_addr_df[JOB_CITY].isna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German state\n",
    "\n",
    "Fill missing job states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Filter missing or invalid states and create a new dataframe\n",
    "missing_state_mask = (job_addr_df[JOB_STATE].isna()\n",
    "                     | job_addr_df[JOB_STATE].str.contains('0-9', regex=True)\n",
    "                     | ~job_addr_df[JOB_STATE].isin(fill_address.bundesland_lst)) \\\n",
    "                     & (job_addr_df[JOB_ZIP].notna() \n",
    "                     | job_addr_df[JOB_CITY].notna())\n",
    "\n",
    "missing_state_df = job_addr_df[missing_state_mask].copy()\n",
    "\n",
    "print_dataframe_length(missing_state_df)\n",
    "missing_state_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Fill missing state\n",
    "missing_state_df = fill_address.fill_missing_state(missing_state_df)\n",
    "\n",
    "column_contains_nan(missing_state_df, JOB_STATE, print_df=False)\n",
    "missing_state_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Update job addresses\n",
    "job_addr_df.update(missing_state_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "job_addr_df[job_addr_df[JOB_STATE].isna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Join dataframes\n",
    "\n",
    "Join dataframes of cleaned company and job addresses with the main dataframe.\n",
    "\n",
    "The main dataframe is updated with only not-NaN values from address dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "column_contains_nan(df, COMPANY_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot num. of NaN values before filling missing values\n",
    "show_nan_counts(df, PLOT_LABELS_WITH_DICT_CLEAN, ymin=13000, ymax=len(df)+500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Update dataframe with company addresses\n",
    "df.update(company_addr_df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot num. of NaN values after filling missing values of company addresses\n",
    "show_nan_counts(df, PLOT_LABELS_WITH_DICT_CLEAN, ymin=13000, ymax=len(df)+500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Update dataframe with job addresses\n",
    "df.update(job_addr_df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot num. of NaN values after filling missing values of job addresses\n",
    "show_nan_counts(df, PLOT_LABELS_WITH_DICT_CLEAN, ymin=13000, ymax=len(df)+500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "column_contains_nan(df, COMPANY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check State values\n",
    "\n",
    "We check counts for each German state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot company states after filling\n",
    "df[COMPANY_STATE].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot job states after filling\n",
    "df[JOB_STATE].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Processed Data\n",
    "\n",
    "The processed data is stored in a csv file on a path:\n",
    "```python\n",
    "../data/processed/jobposting\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "save_dataframe(df, PROCESSED_DATA_DIR, JP_PROCESSED_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (job-posting-linkage)",
   "language": "python",
   "name": "pycharm-90cb392b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
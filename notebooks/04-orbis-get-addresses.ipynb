{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Orbis Data\n",
    "\n",
    "In this notebook, we clean a part of the Orbis dataset containing company addresses.\n",
    "\n",
    "The notebook is organized in the following fashion:\n",
    "\n",
    "0. Import libraries and define constants\n",
    "1. Load parts of Orbis dataset\n",
    "2. Check the data\n",
    "3. Clean addresses\n",
    "4. Translate English names\n",
    "5. German ZIP codes\n",
    "6. Fill missing data\n",
    "7. Clean the result\n",
    "8. Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!pip install modin[all] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ftfy\n",
    "import pyunpack\n",
    "import numpy as np\n",
    "os.environ[\"MODIN_ENGINE\"] = \"dask\"\n",
    "import modin.pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline     \n",
    "sns.set(color_codes=True)\n",
    "\n",
    "import linkage.model.fill_addresses as fa\n",
    "import linkage.model.german_zip_codes as gzc\n",
    "\n",
    "from linkage.model.utils import save_dataframe, read_dataframe\n",
    "from linkage.model.change_dataframe import replace_german_characters, repair_broken_unicode, replace_other_latin_characters\n",
    "from linkage.model.clean_addresses import clean_addresses, replace_english_names\n",
    "from linkage.model.examine_dataframe import contains_all_nan, contains_any_nan, drop_all_nan, count_redundant_spaces\n",
    "from linkage.model.examine_dataframe import column_contains_nan, drop_subset_nan, print_dataframe_length\n",
    "from linkage.visualize.plot import plot_histogram\n",
    "from linkage.visualize.visualize_dataframe import show_nan_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two types of data, all or the first part (part01.rar)\n",
    "# part01 is used for implementation purposes \n",
    "# To check if everything is working as it should\n",
    "TYPE = 'part01'  # 'all' or 'part01'\n",
    "\n",
    "# 'std' for standardized, 'std_dict_40k' for dictionary cleaning with the 40k most common words\n",
    "NOTE = 'std_dict_40k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify paths to data directories\n",
    "INTERMEDIATE_DATA_DIR = \"../data/intermediate/orbis\"\n",
    "PROCESSED_DATA_DIR = f\"../data/processed/orbis/{TYPE}\"\n",
    "\n",
    "# Specifie file names\n",
    "#ORBIS_FILE = \"orbis_german_all_addresses.csv\"\n",
    "ORBIS_FILE = f\"orbis_german_all_addresses_unprocessed_{TYPE}_{NOTE}.csv\"\n",
    "ORBIS_PROCESSED_FILE = f\"orbis_german_all_addresses_processed_{TYPE}_{NOTE}_small.csv\"\n",
    "\n",
    "# Dataframe's index\n",
    "ORBIS_INDEX = 'BvD ID number'\n",
    "\n",
    "# Column names\n",
    "# Good to specify if the column names would change\n",
    "COMPANY_CITY, COMPANY_ZIP, COMPANY_STATE = 'City (native)', 'Postcode', 'Region in country'\n",
    "COMPANY_CITY_INTERNATIONAL, COUNTRY, ISO_CODE = 'City', 'Country', 'Country ISO code',\n",
    "\n",
    "# Columns to take when reading the dataframe from a file\n",
    "USEFUL_COLS = [ORBIS_INDEX, COMPANY_CITY, COMPANY_ZIP, COMPANY_STATE,\n",
    "               COMPANY_CITY_INTERNATIONAL, COUNTRY, ISO_CODE]\n",
    "\n",
    "# Address columns\n",
    "COMPANY_ADDR_COLS = [COMPANY_CITY, COMPANY_ZIP, COMPANY_STATE]\n",
    "COMPANY_ADDR_COLS_ALL = [COMPANY_CITY, COMPANY_ZIP, COMPANY_STATE, COUNTRY, ISO_CODE]\n",
    "\n",
    "# Split columns to lists if numerical or alpha-numerical\n",
    "COMPANY_ADDR_COLS_NAMES = [COMPANY_CITY, COMPANY_STATE]\n",
    "COMPANY_ADDR_COLS_ZIPCODES = [COMPANY_ZIP]\n",
    "\n",
    "# Columns to use to determine and drop duplicates\n",
    "DEDUPLICATION_COLS = [ORBIS_INDEX, COMPANY_CITY]\n",
    "\n",
    "# Labels for plots\n",
    "PLOT_LABELS_ALL = ['Comp. ZIP code', 'Comp. city', 'Comp. city internat.', 'Country', 'ISO code', 'Comp. state']\n",
    "PLOT_LABELS = ['Comp. ZIP code', 'Comp. city', 'Comp. state']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load parts of Orbis dataset\n",
    "\n",
    "The Orbis dataset is stored on path:\n",
    "```python\n",
    "../data/intermediate/orbis/\n",
    "```\n",
    "\n",
    "The data are read into Pandas **DataFrame**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read previously obtained addresses of German companies\n",
    "df = read_dataframe(INTERMEDIATE_DATA_DIR, ORBIS_FILE, ORBIS_INDEX, USEFUL_COLS)\n",
    "print_dataframe_length(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check the data\n",
    "\n",
    "What should be checked:\n",
    "- Columns' type\n",
    "- Number of unique rows\n",
    "- Index\n",
    "- NaN values\n",
    "- Broken Unicode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the dataframe info\n",
    "\n",
    "First, we check the number of columns and rows.\n",
    "\n",
    "We print the column names with their data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose=True , show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for uniqueness and index\n",
    "\n",
    "Then, we look at the uniqueness of values in the individual columns.\n",
    "\n",
    "Next, we check if the data frame has an index. If there is no index, the execution ends with an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the column is unique\n",
    "for i in df.columns:\n",
    "  print(f'{i} is unique: {df[i].is_unique}')\n",
    "\n",
    "# Check the index values\n",
    "# Results in error if there is no index\n",
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check NaN values\n",
    "\n",
    "Here, we check the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_nan_counts(df, PLOT_LABELS_ALL, ymin=0, ymax=len(df)+1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All values are NaN\n",
    "\n",
    "Let's check if some rows are NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_all_nan(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deal with all NaN rows\n",
    "\n",
    "For now, we will drop the rows with only NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_all_nan(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some values are NaN\n",
    "\n",
    "Let's check if some rows have NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "contains_any_nan(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: **BvD ID number** is not unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset index\n",
    "\n",
    "For cleaning and consequent updating of the dataframe, we need to reset the index. Otherwise, the update of the main dataframe by cleaned subdataframe will end in error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broken Unicode\n",
    "\n",
    "It can happen that someone has encoded Unicode with one standard and decoded it with a different one.\n",
    "\n",
    "As a result, some of the characters may be \"broken\".\n",
    "\n",
    "A nice example is ampersand (&) which will decode as &amp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Repair broken unicode\n",
    "repair_broken_unicode(df, COMPANY_ADDR_COLS_NAMES)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace with basic Latin characters\n",
    "\n",
    "Let's check if the dataframe contains any characters other than basic Latin ones and replace them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all rows with other than German alphanumerical characters\n",
    "df[df[COMPANY_CITY].str.contains('[ÄÖÜßÁÉÓÚ]', regex=True) == True].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace German characters\n",
    "\n",
    "Replace German characters with umlaut and ß with their basic Latin equivalents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace characters with umlaut\n",
    "replace_german_characters(df, COMPANY_ADDR_COLS_NAMES)\n",
    "\n",
    "# Check all rows with other than German alphanumerical characters\n",
    "df[df[COMPANY_CITY].str.contains('[ÄÖÜß]', regex=True) == True].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns contain only German characters\n",
    "\n",
    "Let's check if the company names contain different than German characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace á to a etc.\n",
    "replace_other_latin_characters(df, COMPANY_ADDR_COLS_NAMES)\n",
    "\n",
    "# Check all rows with other than latin alphanumerical characters\n",
    "df[df[COMPANY_CITY].str.contains('[ÁÉÓÚ]', regex=True) == True].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduplication\n",
    "\n",
    "We check duplicated records and drop them.\n",
    "\n",
    "We decide duplicates based on the _BvD ID_ and _City (native)_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the duplicated records\n",
    "df[df.duplicated(subset=DEDUPLICATION_COLS, keep=False) == True].sort_values(ORBIS_INDEX).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df.drop_duplicates(subset=DEDUPLICATION_COLS, inplace=True)\n",
    "\n",
    "# Get the new lenght of the dataframe\n",
    "print_dataframe_length(df)\n",
    "\n",
    "# Check again\n",
    "df[df.duplicated(subset=DEDUPLICATION_COLS, keep=False) == True].sort_values(ORBIS_INDEX).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Columns\n",
    "\n",
    "Let's look at the different values of _Country ISO code_ and _Country_ columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unique values of ISO code\n",
    "pd.DataFrame(df[ISO_CODE].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ISO code values different from 'DE'\n",
    "df[df[ISO_CODE] != 'DE'][ISO_CODE].value_counts().plot.pie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot country values different from 'Germany'\n",
    "non_german_df = df[df[COUNTRY] != 'Germany']\n",
    "non_german_df[COUNTRY].value_counts().plot.pie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns containing other than German addresses\n",
    "df.drop(df[df[COUNTRY] != 'Germany'].index, inplace=True)\n",
    "\n",
    "print_dataframe_length(df)\n",
    "\n",
    "# Check for other countries than Germany\n",
    "df[df[COUNTRY] != 'Germany']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant columns 'Country', 'Country ISO code', and 'City' (international)\n",
    "df.drop(labels=[COUNTRY, ISO_CODE, COMPANY_CITY_INTERNATIONAL], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean addresses\n",
    "\n",
    "\n",
    "Clean non-numerical parts of addresses from non-alphabetical characters, group single consecutive letters, and turn names upper case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Clean company addresses\n",
    "for column_name in COMPANY_ADDR_COLS_NAMES:\n",
    "\n",
    "    clean_addresses(df, column_name)\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Translate English names\n",
    "\n",
    "\n",
    "Some cities may be named in English.\n",
    "Translate the English names to their German equivalents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for German city name\n",
    "df[df[COMPANY_CITY].str.contains('MUNICH') == True].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate city native\n",
    "replace_english_names(df, COMPANY_CITY)\n",
    "\n",
    "# Check for German city name\n",
    "df[df[COMPANY_CITY].str.contains('MUNICH') == True].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. German ZIP codes\n",
    "\n",
    "The _German-Zip-Codes.csv_ of German ZIP codes is saved on path\n",
    "```python\n",
    "../data/external/german-zip-codes\n",
    "```\n",
    "\n",
    "We use German-Zip-Codes to fill the missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize class for German-Zip-Codes\n",
    "german_zipcodes = gzc.GermanZipCodes()\n",
    "\n",
    "# Read the standardized dataframe of german zip codes\n",
    "zip_df = german_zipcodes.zip_df\n",
    "zip_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the german-zip-codes dataframe info\n",
    "zip_df.info(verbose=True , show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace the mean of ZIP codes\n",
    "\n",
    "Because we used the mean of the ZIP codes in the previous step, we do not want to use the full ZIP to avoid confusion.\n",
    "\n",
    "We replace the last 3 numbers of ZIP codes with 'xxx'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_mean_df = german_zipcodes.zip_mean_df\n",
    "zip_mean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fill missing data\n",
    "\n",
    "Here, we try to fill missing parts of company addresses using other non-missing values of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize class for cleaning data\n",
    "fill_address = fa.FillAddress(df, zip_df, zip_mean_df, COMPANY_ZIP, COMPANY_CITY, COMPANY_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ZIP codes\n",
    "\n",
    "Fill missing company ZIP codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter missing or invalid zipcodes and create a new dataframe\n",
    "missing_zip_mask = (df[COMPANY_ZIP].isna() \\\n",
    "                    | df[COMPANY_ZIP].str.contains('[a-zA-Z]', regex=True) == True) \\\n",
    "                    & df[COMPANY_CITY].notna()\n",
    "\n",
    "missing_zip_df = df[missing_zip_mask].copy()\n",
    "missing_zip_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing zipcode\n",
    "missing_zip_df = fill_address.fill_missing_zipcode(missing_zip_df)\n",
    "missing_zip_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Update main dataframe\n",
    "df.update(missing_zip_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[COMPANY_ZIP].isna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City (native)\n",
    "\n",
    "Fill missing company cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[COMPANY_CITY].isna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Missing City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter missing or invalid cities and create a new dataframe\n",
    "missing_city_mask = (df[COMPANY_CITY].isna() \\\n",
    "                     | (df[COMPANY_CITY].str.contains('[0-9]', regex=True) == True)) \\\n",
    "                     & df[COMPANY_ZIP].notna()\n",
    "\n",
    "missing_city_df = df[missing_city_mask].copy()\n",
    "\n",
    "missing_city_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Fill missing city\n",
    "missing_city_df = fill_address.fill_missing_city(missing_city_df)\n",
    "missing_city_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Update main dataframe\n",
    "df.update(missing_city_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[COMPANY_CITY].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Unclear German state\n",
    "\n",
    "Change Region in country to Bundesland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter unclear states (containing a '|') and create a new dataframe\n",
    "unclear_state_mask = df[COMPANY_STATE].notna() & df[COMPANY_STATE].str.contains('|', regex=False)\n",
    "\n",
    "unclear_state_df = df[unclear_state_mask].copy()\n",
    "\n",
    "unclear_state_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fill unclear state\n",
    "unclear_state_df = fill_address.fill_unclear_state(unclear_state_df)\n",
    "unclear_state_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update main dataframe\n",
    "df.update(unclear_state_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[COMPANY_STATE].notna() & df[COMPANY_STATE].str.contains('|', regex=False))].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### German State\n",
    "\n",
    "Fill missing company states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[COMPANY_STATE] = df[COMPANY_STATE].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Filter missing or invalid states and create a new dataframe\n",
    "missing_state_mask = (df[COMPANY_STATE].isna() \n",
    "                      | df[COMPANY_STATE].str.contains('0-9', regex=True)\n",
    "                     | ~df[COMPANY_STATE].isin(fill_address.bundesland_lst)) \\\n",
    "                      & (df[COMPANY_ZIP].notna() \n",
    "                      | df[COMPANY_CITY].notna())\n",
    "\n",
    "missing_state_df = df[missing_state_mask].copy()\n",
    "missing_state_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Fill missing state\n",
    "missing_state_df = fill_address.fill_missing_state(missing_state_df)\n",
    "missing_state_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update main dataframe\n",
    "df.update(missing_state_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[COMPANY_STATE].isna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Clean the result\n",
    "\n",
    "Here, we again:\n",
    "\n",
    "- Deduplicate records\n",
    "- Check NaN values\n",
    "- Set an index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduplication\n",
    "\n",
    "After filling missing values, a new duplicated records may appear. Here, we will deal with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the duplicated records\n",
    "df[df.duplicated(subset=DEDUPLICATION_COLS, keep=False) == True].sort_values(ORBIS_INDEX).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df.drop_duplicates(subset=DEDUPLICATION_COLS, inplace=True)\n",
    "\n",
    "# Check again\n",
    "df[df.duplicated(subset=DEDUPLICATION_COLS, keep=False) == True].sort_values(ORBIS_INDEX).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set an index\n",
    "\n",
    "In _1. Load parts of Orbis dataset_, we read the data without setting the _BvD ID_ as an index.\n",
    "\n",
    "We set the index now, so the data saved to file do not contain an additional column with previously used index (line number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set an index\n",
    "df.set_index(ORBIS_INDEX, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dataframe_length(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check NaN values\n",
    "\n",
    "Here, we check the remaining missing data after the cleaning.\n",
    "\n",
    "We drop the records, which contain only NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot num. of NaN values after filling missing values before dropping NaN values\n",
    "show_nan_counts(df, PLOT_LABELS, ymin=0, ymax=len(df)+1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_all_nan(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_all_nan(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot num. of NaN values after filling missing values after dropping NaN values\n",
    "show_nan_counts(df, PLOT_LABELS, ymin=0, ymax=len(df)+1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check State values\n",
    "\n",
    "We check counts for each German state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot company states after filling\n",
    "df[COMPANY_STATE].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save processed data\n",
    "\n",
    "The processed data is stored in a csv file on path:\n",
    "```python\n",
    "../data/processed/orbis\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataframe(df, PROCESSED_DATA_DIR, ORBIS_PROCESSED_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "#%"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Clean Orbis Data\n",
    "\n",
    "In this notebook, we clean a part of the Orbis dataset containing company names.\n",
    "\n",
    "The notebook is organized in the following fashion:\n",
    "\n",
    "0. Import libraries and define constants\n",
    "1. Load parts of Orbis dataset\n",
    "2. Check the data\n",
    "3. Clean company names\n",
    "4. Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ftfy\n",
    "import pyunpack\n",
    "import rarfile\n",
    "os.environ[\"MODIN_ENGINE\"] = \"dask\"\n",
    "import modin.pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline     \n",
    "sns.set(color_codes=True)\n",
    "\n",
    "from linkage.model.utils import save_dataframe, read_dataframe\n",
    "from linkage.model.change_dataframe import replace_german_characters, repair_broken_unicode, replace_other_latin_characters\n",
    "from linkage.model.clean_names import clean_names, clean_names_with_dictionary\n",
    "from linkage.model.examine_dataframe import contains_all_nan, contains_any_nan, drop_all_nan, count_redundant_spaces\n",
    "from linkage.model.examine_dataframe import column_contains_nan, drop_subset_nan, print_dataframe_length\n",
    "from linkage.visualize.plot import plot_histogram\n",
    "from linkage.visualize.visualize_dataframe import show_nan_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two types of data, all or the first part (part01.rar)\n",
    "# part01 is used for implementation purposes \n",
    "# To check if everything is working as it sould\n",
    "TYPE = 'all'  # 'all' or 'part01'\n",
    "\n",
    "# 'std' for standardized, 'std_dict_40k' for dictionary cleaning with the 40k most common words\n",
    "NOTE = 'std'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify paths to data directories\n",
    "INTERMEDIATE_DATA_DIR = \"../data/intermediate/orbis\"\n",
    "PROCESSED_DATA_DIR = f\"../data/processed/orbis/{TYPE}\"\n",
    "\n",
    "# Specifie file names\n",
    "#ORBIS_FILE = \"orbis_german_bvid_name_unprocessed_part01.csv\"  # TODO\n",
    "ORBIS_FILE = f\"orbis_german_bvid_name_unprocessed_{TYPE}.csv\"\n",
    "ORBIS_PROCESSED_FILE = f\"orbis_german_bvid_name_processed_{TYPE}.csv\"\n",
    "\n",
    "# Dataframe's index\n",
    "ORBIS_INDEX = 'BvD ID number'\n",
    "\n",
    "# Column names\n",
    "# Good to specify if the column names would change\n",
    "COMPANY_NAME = 'NAME'\n",
    "\n",
    "# Columns to take when reading the dataframe from a file\n",
    "USEFUL_COLS = [ORBIS_INDEX, COMPANY_NAME]\n",
    "\n",
    "# Additional columns\n",
    "COMPANY_NAME_STANDARDIZED = 'company_standard'\n",
    "COMPANY_NAME_DICT_CLEANED = 'company_dict_clean'\n",
    "\n",
    "# Labels for plots\n",
    "PLOT_LABELS = ['Comp. name']\n",
    "PLOT_LABELS_WITH_DICT_CLEAN = ['Comp. name', 'Comp. name stand.', 'Comp. name dict. clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load parts of Orbis dataset\n",
    "\n",
    "The Orbis dataset is stored on path:\n",
    "```python\n",
    "../data/intermediate/orbis/\n",
    "```\n",
    "\n",
    "The data are read into Pandas **DataFrame**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read previously obtained German companies\n",
    "df = read_dataframe(INTERMEDIATE_DATA_DIR, ORBIS_FILE, None, USEFUL_COLS)\n",
    "print_dataframe_length(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check the data\n",
    "\n",
    "What should be checked:\n",
    "- Columns' type\n",
    "- Number of unique rows\n",
    "- Index\n",
    "- NaN values\n",
    "- Broken Unicode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the dataframe info\n",
    "\n",
    "First, we check the number of columns and rows.\n",
    "\n",
    "We print the column names with their data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names, the number of the columns, the number of rows\n",
    "df.info(verbose=True , show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for uniqueness and index\n",
    "\n",
    "Then, we look at the uniqueness of values in the individual columns.\n",
    "\n",
    "Next, we check if the data frame has an index. If there is no index, the execution ends with an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the column is unique\n",
    "for i in df.columns:\n",
    "  print(f'{i} is unique: {df[i].is_unique}')\n",
    "\n",
    "# Check the index values\n",
    "# Results in error if there is no index\n",
    "df.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for NaN values\n",
    "\n",
    "Here, we check the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_nan_counts(df, PLOT_LABELS, ymin=0, ymax=len(df)+500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All values are NaN\n",
    "\n",
    "Let's check if some rows are NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_all_nan(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deal with all NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_all_nan(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some values are NaN\n",
    "\n",
    "Let's check if some rows are NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_any_nan(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check broken Unicode\n",
    "\n",
    "It can happen that someone has encoded Unicode with one standard and decoded it with a different one.\n",
    "\n",
    "As a result, some of the characters may be \"broken\".\n",
    "\n",
    "A nice example is ampersand (&) which will decode as &amp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for broken Unicode\n",
    "df[df[COMPANY_NAME].str.contains('&amp', regex=True, case=False)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repair broken Unicode\n",
    "\n",
    "The library ftfy (fixes text for you) will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repair broken unicode\n",
    "repair_broken_unicode(df, COMPANY_NAME)\n",
    "    \n",
    "# Look again for the broken ampersand\n",
    "df[df[COMPANY_NAME].str.contains('&amp;', regex=True, case=False) == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace with basic Latin characters\n",
    "\n",
    "Let's check if the dataframe contains any characters other than basic Latin ones and replace them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all rows with other than German alphanumerical characters\n",
    "df[df[COMPANY_NAME].str.contains('[ÄÖÜßÁÉÓÚ]', regex=True) == True].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace German characters\n",
    "\n",
    "Replace German characters with umlaut and ß with their basic Latin equivalents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace characters with umlaut\n",
    "replace_german_characters(df, COMPANY_NAME)\n",
    "\n",
    "# Check all rows with other than German alphanumerical characters\n",
    "df[df[COMPANY_NAME].str.contains('[ÄÖÜß]', regex=True) == True].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns contain only German characters\n",
    "\n",
    "Let's check if the company names contain different than German characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace á to a etc.\n",
    "replace_other_latin_characters(df, COMPANY_NAME)\n",
    "\n",
    "# Check all rows with other than latin alphanumerical characters\n",
    "df[df[COMPANY_NAME].str.contains('[ÁÉÓÚ]', regex=True) == True].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the range of the values\n",
    "\n",
    "We look at the values of company names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(df, column_name=COMPANY_NAME, title='Count by company name before standardization and deduplication.', \n",
    "               ylabel='Num. of appearances per company name', xlabel='Company name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduplication\n",
    "\n",
    "We check duplicated records and drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the duplicated records\n",
    "df[df.duplicated(subset=USEFUL_COLS, keep=False) == True].sort_values(ORBIS_INDEX).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df.drop_duplicates(subset=USEFUL_COLS, inplace=True)\n",
    "\n",
    "# Get the new lenght of the dataframe\n",
    "print_dataframe_length(df)\n",
    "\n",
    "# Check again\n",
    "df[df.duplicated(subset=USEFUL_COLS, keep=False) == True].sort_values(ORBIS_INDEX).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the range of the values\n",
    "\n",
    "We look at the values of company names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(df, column_name=COMPANY_NAME, title='Count by company name deduplicated before standardization.', \n",
    "               ylabel='Num. of appearances per company name', xlabel='Company name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean company names\n",
    "\n",
    "Here, we will clean the company names.\n",
    "\n",
    "In general, we will remove:\n",
    "- Redundant characters\n",
    "- Redundant individually standing numbers\n",
    "- Redundant white characters\n",
    "\n",
    "We will apply:\n",
    "- PDP standardization routines\n",
    "- Dictionary cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove redundant words\n",
    "\n",
    "Here, we standardize company names and apply PDP: \n",
    "\n",
    "PDP standardization routines:\n",
    "\n",
    "0. Remove non-alphanumerical characters.\n",
    "\n",
    "1. Change things to shortcuts\n",
    "\n",
    "2. Remove the shortcuts\n",
    "\n",
    "3. Remove corporate names and non-corporate\n",
    "\n",
    "4. Combine abbreviations and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[COMPANY_NAME_STANDARDIZED] = df[COMPANY_NAME]\n",
    "df.head()\n",
    "\n",
    "# Clean company names from redundant words\n",
    "clean_names(df, column_name=COMPANY_NAME_STANDARDIZED)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check empty names after cleaning\n",
    "\n",
    "Let's check if some of the values resulted in empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which company names resulted into empty string after cleaning\n",
    "empty_name_filter = df[df[COMPANY_NAME] == '']\n",
    "\n",
    "empty_name_df = df[df.index.isin(empty_name_filter.index)]\n",
    "\n",
    "empty_name_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill empty \n",
    "\n",
    "Fill the empty company name values with their original version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the values where the empty company name is\n",
    "empty_name_df[COMPANY_NAME_STANDARDIZED] = empty_name_df[COMPANY_NAME]\n",
    "\n",
    "# Clean company names without removing redundant words\n",
    "clean_names(empty_name_df, column_name=COMPANY_NAME_STANDARDIZED, remove_redundant=False)\n",
    "\n",
    "# Update the values where the empty company name is\n",
    "df.update(empty_name_df)\n",
    "\n",
    "df[df.index.isin(empty_name_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all rows with other than German alphanumerical characters\n",
    "df[df[COMPANY_NAME_STANDARDIZED].str.contains('[^a-zA-Z0-9 ]', regex=True) == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Company names do not contains space on the beginning and the end, or double spaces\n",
    "\n",
    "Cleaning of the names should have removed all the redundant spaces created during cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_redundant_spaces(df, COMPANY_NAME_STANDARDIZED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove redundant words using dictionary\n",
    "\n",
    "We use a dictionary containing the 40k most common German words.\n",
    "\n",
    "We use this step to achieve similar results we achieve when cleaning the JobPostings dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[COMPANY_NAME_DICT_CLEANED] = df[COMPANY_NAME_STANDARDIZED]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean names with a dictionary\n",
    "#clean_names_with_dictionary(df, column_name=COMPANY_NAME_DICT_CLEANED) # TODO: uncomment\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the results\n",
    "\n",
    "Let's plot value counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Counts after standardization\n",
    "company_counts_df = df[COMPANY_NAME_STANDARDIZED].value_counts().copy()\n",
    "plot_histogram(company_counts_df, title='Count by company name after standardization.', \n",
    "               ylabel='Num. of appearances per company name', xlabel='Company name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Counts after standardization and dictionary cleaning\n",
    "company_dict_clean_counts_df = df[COMPANY_NAME_DICT_CLEANED].value_counts().copy()\n",
    "plot_histogram(company_dict_clean_counts_df, title='Count by company name after standardization and dictionary cleaning.', \n",
    "               ylabel='Num. of appearances per company name', xlabel='Company name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save company appearences to CSV\n",
    "\n",
    "To check possible missed company types, e.g. GGMBH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_standard_counts_df_file = \"orbis_company_standard_value_counts.csv\"\n",
    "company_dict_clean_counts_df_file = \"orbis_company_copy_value_counts.csv\"\n",
    "\n",
    "# Save dataframe to a csv file\n",
    "save_dataframe(company_counts_df, INTERMEDIATE_DATA_DIR, company_standard_counts_df_file)\n",
    "save_dataframe(company_dict_clean_counts_df, INTERMEDIATE_DATA_DIR, company_dict_clean_counts_df_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set an index\n",
    "\n",
    "In _1. Load parts of Orbis dataset_, we read the data without setting the _BvD ID_ as an index.\n",
    "\n",
    "We set the index now, so the data saved to file do not contain an additional column with previously used index (line number).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an index\n",
    "df.set_index(ORBIS_INDEX, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save processed data\n",
    "\n",
    "The processed data is stored in a csv file on path:\n",
    "```python\n",
    "../data/processed/orbis/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "save_dataframe(df, PROCESSED_DATA_DIR, ORBIS_PROCESSED_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
